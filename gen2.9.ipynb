{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  random_split, Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl \n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBarTheme\n",
    "import torchdyn \n",
    "from torchdyn.core import NeuralODE\n",
    "from torchgan.losses import  MinimaxGeneratorLoss, MinimaxDiscriminatorLoss\n",
    "from ignite.metrics import FID\n",
    "\n",
    "import os \n",
    "import cv2 \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeData(Dataset):\n",
    "    def __init__(self, path, inpshape=(28, 36), outshape=(224, 288), datasize:int=1000):\n",
    "        super(GenerativeData, self).__init__()\n",
    "        self.path = path \n",
    "        self.inpshape = inpshape\n",
    "        self.outshape = outshape\n",
    "        self.datasize = datasize\n",
    "        self.inpimg, self.outimg = self.datareader()\n",
    "\n",
    "    def preprocess(self, image, imagesize):\n",
    "        process = T.Compose([T.ToTensor(), \n",
    "                             T.Resize(imagesize),\n",
    "                             T.Normalize(mean=(0.5, 0.5, 0.5), std=(1, 1, 1))])\n",
    "        return process(np.array(image, dtype=np.float32))\n",
    "\n",
    "    def datareader(self):\n",
    "        X = []\n",
    "        Y = []\n",
    "        files = os.listdir(self.path)\n",
    "        for c, file in enumerate(files):\n",
    "            image = cv2.imread(self.path+file)\n",
    "            X.append(self.preprocess(image, self.inpshape))\n",
    "            Y.append(self.preprocess(image, self.outshape))\n",
    "            if c>self.datasize:\n",
    "                break\n",
    "\n",
    "        return X, Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inpimg)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inpimg = th.tensor(self.inpimg[idx], dtype=th.float32)\n",
    "        outimg = th.tensor(self.outimg[idx], dtype=th.float32)\n",
    "        return {\"inputs\": inpimg, \"outputs\": outimg}             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorBlock(nn.Module):\n",
    "    def __init__(self, num:int, infilter:int, outfilter:int, kernel:int, moment:float=0.9, alpha:float=0.1):\n",
    "        super(DiscriminatorBlock, self).__init__()\n",
    "        self.conv = nn.ModuleList(modules=[nn.Conv2d(infilter, outfilter, kernel)])\n",
    "        self.norm = nn.ModuleList(modules=[nn.BatchNorm2d(outfilter, momentum=moment)])\n",
    "        self.pad  = nn.ZeroPad2d(int((kernel-1)//2))\n",
    "        self.act = nn.LeakyReLU(alpha)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        for _ in range(num-1):\n",
    "            self.conv.append(nn.Conv2d(outfilter, outfilter, kernel))\n",
    "            self.norm.append(nn.BatchNorm2d(outfilter, momentum=moment))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv, norm in zip(self.conv, self.norm):\n",
    "            x = self.act(norm(conv(self.pad(x))))\n",
    "        return self.pool(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num:list=[1, 1, 1, 1], filter:int=32, start_kernel:int=5, kernel:int=3, \n",
    "                 moment:float=0.9, alpha:float=0.1, dense:int=64, gf:float=2.0, drop:float=0.2):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.convblock = nn.ModuleList(modules=[DiscriminatorBlock(1, 3, filter, start_kernel, moment, alpha)])\n",
    "        for n in num:\n",
    "            self.convblock.append(DiscriminatorBlock(n, filter, int(filter*gf), kernel, moment, alpha))\n",
    "            filter = int(filter*gf)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flat = nn.Flatten()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.dense = nn.Linear(filter, dense)\n",
    "        self.final = nn.Linear(dense, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for convblock in self.convblock:\n",
    "            x = convblock(x)\n",
    "        x = self.flat(self.pool(x))\n",
    "        x = F.relu(self.dense(x))\n",
    "        return F.softmax(self.final(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorBlock(nn.Module):\n",
    "    def __init__(self, num:int, infilter:int, outfilter:int, kernel:int, moment:float=0.9, alpha:float=0.1):\n",
    "        super(GeneratorBlock, self).__init__()\n",
    "        pad  = int((kernel-1)//2)\n",
    "        self.conv = nn.ModuleList(modules=[nn.ConvTranspose2d(infilter, outfilter, kernel, stride=2, padding=pad)])\n",
    "        self.norm = nn.ModuleList(modules=[nn.BatchNorm2d(outfilter, momentum=moment)])\n",
    "        self.act = nn.LeakyReLU(alpha)\n",
    "        \n",
    "        for _ in range(num-1):\n",
    "            self.conv.append(nn.ConvTranspose2d(outfilter, outfilter, kernel, padding=pad))\n",
    "            self.norm.append(nn.BatchNorm2d(outfilter, momentum=moment))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv, norm in zip(self.conv, self.norm):\n",
    "            x = self.act(norm(conv(x)))     \n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, num:list=[2, 2, 2], filter:int=32, start_kernel:int=5, kernel:int=3, \n",
    "                 moment:float=0.9, alpha:float=0.1, gf:float=2.0):\n",
    "        super(Generator, self).__init__()\n",
    "        self.convblock = [GeneratorBlock(1, 3, filter, start_kernel, moment, alpha)] \n",
    "        for n in num:\n",
    "            self.convblock.append(GeneratorBlock(n, filter, int(filter*gf), kernel, moment, alpha))\n",
    "            filter = int(filter*gf)\n",
    "\n",
    "        self.convblock.append(GeneratorBlock(1, filter, 3, kernel, moment, alpha))\n",
    "        self.convblock = nn.ModuleList(modules=self.convblock)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for convblock in self.convblock:\n",
    "            x = convblock(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(LightningModule):\n",
    "    def __init__(self, dataset:Dataset, batchsize:int=10, validbatchsize:int=10, split:float=0.3, lr:float=1e-4):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = Discriminator()\n",
    "        self.generator = Generator()\n",
    "        self.batchsize = batchsize\n",
    "        self.validbatchsize = validbatchsize\n",
    "        split = int(dataset.__len__()*split)\n",
    "        self.traindata, self.validdata = random_split(dataset, [split, dataset.__len__()-split])\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self,gen, dis):\n",
    "        true = self.discriminator(dis)\n",
    "        gen = self.generator(gen)\n",
    "        fake = self.discriminator(gen)\n",
    "        return fake, true\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        gen = batch[\"inputs\"]\n",
    "        dis = batch[\"outputs\"]\n",
    "        fake, true = self(gen, dis)\n",
    "        gloss = MinimaxGeneratorLoss()(fake)\n",
    "        dloss = MinimaxDiscriminatorLoss()(true, fake)\n",
    "        # fid = FID()(dis, fake)\n",
    "        cur_lr = self.trainer.optimizers[0].param_groups[0]['lr']\n",
    "        self.log(\"lr\", cur_lr, prog_bar=True, on_step=True)\n",
    "        self.log_dict({\"generator_loss\": gloss, \"discriminator_loss\": dloss}, on_step=True, on_epoch=True, prog_bar=True, logger=False)\n",
    "        if optimizer_idx==0:\n",
    "            return gloss\n",
    "        else:\n",
    "            return dloss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        gen = batch[\"inputs\"]\n",
    "        dis = batch[\"outputs\"]\n",
    "        fake, true = self(gen, dis)\n",
    "        gloss = MinimaxGeneratorLoss()(fake)\n",
    "        dloss = MinimaxDiscriminatorLoss()(true, fake)\n",
    "        # fid = FID()(dis, fake)\n",
    "        cur_lr = self.trainer.optimizers[0].param_groups[0]['lr']        \n",
    "        self.log(\"lr\", cur_lr, prog_bar=True, on_step=True)\n",
    "        self.log_dict({\"generator_loss\": gloss, \"discriminator_loss\": dloss}, on_step=True, on_epoch=True, prog_bar=True, logger=False)\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        goptim = optim.Adam(self.generator.parameters(), lr=self.lr)\n",
    "        doptim = optim.Adam(self.discriminator.parameters(), lr=self.lr)\n",
    "        gsch = optim.lr_scheduler.StepLR(goptim, step_size  = 10 , gamma = 0.1)\n",
    "        dsch = optim.lr_scheduler.StepLR(doptim, step_size  = 10 , gamma = 0.1)\n",
    "        return  [{\"optimizer\": goptim, \"lr_schedular\": {\"schedular\":gsch, \"monitor\": \"generator_loss\"}} , \n",
    "                 {\"optimizer\": doptim, \"lr_schedular\": {\"schedular\":dsch, \"monitor\": \"discriminator_loss\"}}]\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.traindata, batch_size=self.batchsize, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.validdata, batch_size=self.validbatchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\suyash\\Downloads\\SeaGAN\\seacreature_images_transformed\\seacreature_images_transformed/'\n",
    "dataset = GenerativeData(path)\n",
    "\n",
    "# progress_bar = RichProgressBar(theme=RichProgressBarTheme(description=\"blue\",progress_bar=\"green_yellow\",progress_bar_finished=\"green1\",\n",
    "#         progress_bar_pulse=\"#6206E0\",batch_progress=\"blue\",  time=\"black\",processing_speed=\"black\",metrics=\"black\", ),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = GAN(dataset)\n",
    "trainer = pl.Trainer(min_epochs=30, max_epochs=30)\n",
    "trainer.fit(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
