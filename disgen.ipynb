{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pytorch_lightning as pl \n",
    "import lightning as L\n",
    "import torchvision.transforms as T\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBarTheme\n",
    "from torchgan.losses import  MinimaxGeneratorLoss, MinimaxDiscriminatorLoss\n",
    "from ignite.metrics import FID\n",
    "\n",
    "import os \n",
    "import cv2 \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeData(Dataset):\n",
    "    def __init__(self, path, inpshape=(28, 36), outshape=(224, 288), datasize:int=1000):\n",
    "        super(GenerativeData, self).__init__()\n",
    "        self.path = path \n",
    "        self.inpshape = inpshape\n",
    "        self.outshape = outshape\n",
    "        self.datasize = datasize\n",
    "        self.inpimg, self.outimg = self.datareader()\n",
    "\n",
    "    def preprocess(self, image, imagesize):\n",
    "        process = T.Compose([T.ToTensor(), \n",
    "                             T.Resize(imagesize),\n",
    "                             T.Normalize(mean=(0.5, 0.5, 0.5), std=(1, 1, 1))])\n",
    "        return process(np.array(image, dtype=np.float32))\n",
    "\n",
    "    def datareader(self):\n",
    "        X = []\n",
    "        Y = []\n",
    "        files = os.listdir(self.path)\n",
    "        for c, file in enumerate(files):\n",
    "            image = cv2.imread(self.path+file)\n",
    "            X.append(self.preprocess(image, self.inpshape))\n",
    "            Y.append(self.preprocess(image, self.outshape))\n",
    "            if c>self.datasize:\n",
    "                break\n",
    "\n",
    "        return X, Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inpimg)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inpimg = th.tensor(self.inpimg[idx], dtype=th.float32)\n",
    "        outimg = th.tensor(self.outimg[idx], dtype=th.float32)\n",
    "        return {\"inputs\": inpimg, \"outputs\": outimg}             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorBlock(nn.Module):\n",
    "    def __init__(self, num:int, infilter:int, outfilter:int, kernel:int, moment:float=0.9, alpha:float=0.1):\n",
    "        super(DiscriminatorBlock, self).__init__()\n",
    "        self.conv = nn.ModuleList(modules=[nn.Conv2d(infilter, outfilter, kernel)])\n",
    "        self.norm = nn.ModuleList(modules=[nn.BatchNorm2d(outfilter, momentum=moment)])\n",
    "        self.pad  = nn.ZeroPad2d(int((kernel-1)//2))\n",
    "        self.act = nn.LeakyReLU(alpha)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        for _ in range(num-1):\n",
    "            self.conv.append(nn.Conv2d(outfilter, outfilter, kernel))\n",
    "            self.norm.append(nn.BatchNorm2d(outfilter, momentum=moment))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv, norm in zip(self.conv, self.norm):\n",
    "            x = self.act(norm(conv(self.pad(x))))\n",
    "        return self.pool(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num:list=[1, 1, 1, 1], filter:int=32, start_kernel:int=5, kernel:int=3, \n",
    "                 moment:float=0.9, alpha:float=0.1, dense:int=64, gf:float=2.0, drop:float=0.2):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.convblock = nn.ModuleList(modules=[DiscriminatorBlock(1, 3, filter, start_kernel, moment, alpha)])\n",
    "        for n in num:\n",
    "            self.convblock.append(DiscriminatorBlock(n, filter, int(filter*gf), kernel, moment, alpha))\n",
    "            filter = int(filter*gf)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flat = nn.Flatten()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.dense = nn.Linear(filter, dense)\n",
    "        self.final = nn.Linear(dense, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for convblock in self.convblock:\n",
    "            x = convblock(x)\n",
    "        x = self.flat(self.pool(x))\n",
    "        x = F.relu(self.dense(x))\n",
    "        return F.softmax(self.final(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorBlock(nn.Module):\n",
    "    def __init__(self, num:int, infilter:int, outfilter:int, kernel:int, moment:float=0.9, alpha:float=0.1):\n",
    "        super(GeneratorBlock, self).__init__()\n",
    "        pad  = int((kernel-1)//2)\n",
    "        self.conv = nn.ModuleList(modules=[nn.ConvTranspose2d(infilter, outfilter, kernel, stride=2, padding=pad)])\n",
    "        self.norm = nn.ModuleList(modules=[nn.BatchNorm2d(outfilter, momentum=moment)])\n",
    "        self.act = nn.LeakyReLU(alpha)\n",
    "        \n",
    "        for _ in range(num-1):\n",
    "            self.conv.append(nn.ConvTranspose2d(outfilter, outfilter, kernel, padding=pad))\n",
    "            self.norm.append(nn.BatchNorm2d(outfilter, momentum=moment))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv, norm in zip(self.conv, self.norm):\n",
    "            x = self.act(norm(conv(x)))     \n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, num:list=[2, 2, 2], filter:int=32, start_kernel:int=5, kernel:int=3, \n",
    "                 moment:float=0.9, alpha:float=0.1, gf:float=2.0):\n",
    "        super(Generator, self).__init__()\n",
    "        self.convblock = [GeneratorBlock(1, 3, filter, start_kernel, moment, alpha)] \n",
    "        for n in num:\n",
    "            self.convblock.append(GeneratorBlock(n, filter, int(filter*gf), kernel, moment, alpha))\n",
    "            filter = int(filter*gf)\n",
    "\n",
    "        self.convblock.append(GeneratorBlock(1, filter, 3, kernel, moment, alpha))\n",
    "        self.convblock = nn.ModuleList(modules=self.convblock)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for convblock in self.convblock:\n",
    "            x = convblock(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(L.LightningModule):\n",
    "    def __init__(self, dataset:Dataset, generator:nn.Module, discriminator:nn.Module, batch:int=10, valid:int=10, split:float=0.7, lr:float=1e-4):\n",
    "        super().__init__() \n",
    "        self.automatic_optimization = False\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.batch = batch\n",
    "        self.valid = valid \n",
    "        self.lr = lr\n",
    "        split = int(dataset.__len__()*split)\n",
    "        self.traindata, self.validdata = random_split(dataset, [split, dataset.__len__()-split])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.generator(x)\n",
    "    \n",
    "    def loss(self, ypred, ytrue):\n",
    "        return F.binary_cross_entropy(ypred, ytrue)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        gen = batch[\"inputs\"]\n",
    "        dis = batch[\"outputs\"]\n",
    "\n",
    "        goptim, doptim = self.optimizers()\n",
    "        self.toggle_optimizer(goptim)\n",
    "        self.genimage = self(gen)\n",
    "\n",
    "        valid = th.ones(gen.size(0), 1)\n",
    "        valid = valid.type_as(gen)\n",
    "\n",
    "        gloss = self.loss(self.discriminator(self(gen)), valid)\n",
    "        self.log(\"g_loss\", gloss, on_step=True, on_epoch=True, prog_bar=True, logger=False)\n",
    "        self.manual_backward(gloss)\n",
    "        goptim.step()\n",
    "        goptim.zero_grad()\n",
    "        self.untoggle_optimizer(goptim)\n",
    "\n",
    "        valid = th.ones(gen.size(0), 1)\n",
    "        valid = valid.type_as(gen)\n",
    "        real_loss = self.loss(self.discriminator(dis), valid)\n",
    "\n",
    "        fake = th.zeros(gen.size(0), 1)\n",
    "        self.toggle_optimizer(doptim)\n",
    "        fake_loss = self.loss(self.discriminator(self(gen).detach()), fake)\n",
    "\n",
    "        dloss = (fake_loss+real_loss)/2\n",
    "        self.log(\"d_loss\", dloss, on_step=True, on_epoch=True, prog_bar=True, logger=False)\n",
    "        self.manual_backward(dloss)\n",
    "        doptim.step()\n",
    "        doptim.zero_grad()\n",
    "        self.untoggle_optimizer(doptim)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        goptim = optim.Adam(self.generator.parameters(), lr=self.lr)\n",
    "        doptim = optim.Adam(self.discriminator.parameters(), lr=self.lr)\n",
    "        return [doptim, goptim], []\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.traindata, batch_size=self.batch, shuffle=True)\n",
    "    \n",
    "    # def val_dataloader(self):\n",
    "    #     return DataLoader(self.validdata, batch_size=self.valid, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | discriminator | Discriminator | 1.6 M \n",
      "1 | generator     | Generator     | 1.2 M \n",
      "------------------------------------------------\n",
      "2.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 M     Total params\n",
      "11.113    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f563c4950e09410d90591a323dfcf708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = r'C:\\Users\\suyash\\Downloads\\SeaGAN\\seacreature_images_transformed\\seacreature_images_transformed/'\n",
    "dataset = GenerativeData(path, datasize=100)\n",
    "dis = Discriminator()\n",
    "gen = Generator()\n",
    "\n",
    "progress_bar = RichProgressBar(theme=RichProgressBarTheme(description=\"blue\",progress_bar=\"green_yellow\",progress_bar_finished=\"green1\",\n",
    "        progress_bar_pulse=\"#6206E0\",batch_progress=\"blue\",  time=\"black\",processing_speed=\"black\",metrics=\"black\", ),)\n",
    "learn = GAN(dataset, gen, dis, batch=2, valid=2)\n",
    "trainer = L.Trainer(min_epochs=30, max_epochs=30)\n",
    "trainer.fit(learn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
